---
layout: post
title: "Transformeræ¶æ„è¯¦è§£"
subtitle: "æ·±å…¥ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ä½ç½®ç¼–ç "
date: 2024-09-01
author: "Feng Yu"
header-img: "img/post-bg-recitewords.jpg"
catalog: true
permalink: /2024/09/01/Transformeræ¶æ„è¯¦è§£/
tags:
  - å¤§è¯­è¨€æ¨¡å‹
  - Transformer
  - æ³¨æ„åŠ›æœºåˆ¶
  - æ·±åº¦å­¦ä¹ 
categories:
  - Deep Learning
  - NLP
---

> Transformeræ¶æ„çš„è¯ç”Ÿæ ‡å¿—ç€æ·±åº¦å­¦ä¹ è¿›å…¥æ–°çºªå…ƒã€‚2017å¹´ï¼ŒGoogleå›¢é˜Ÿåœ¨è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºTransformerï¼Œå½»åº•é¢ è¦†äº†åºåˆ—å»ºæ¨¡çš„ä¼ ç»ŸèŒƒå¼ã€‚å®ƒä¸ä»…å‚¬ç”Ÿäº†BERTã€GPTç­‰é‡Œç¨‹ç¢‘æ¨¡å‹ï¼Œæ›´æˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºçŸ³ã€‚

---

## ä¸€ã€æ ¸å¿ƒæ¶æ„

**Encoderå—**ç»“æ„ï¼š
```
Input Embedding + Positional Encoding
    â†“
Multi-Head Self-Attention
    â†“
Add & Norm
    â†“
Feed-Forward Network
    â†“
Add & Norm
```

**å®Œæ•´å®ç°**ï¼š
```python
import torch.nn as nn

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + LayerNorm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))
        
        # FFN + æ®‹å·®è¿æ¥ + LayerNorm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))
        
        return x
```

**Decoderå—**å¢åŠ äº¤å‰æ³¨æ„åŠ›ï¼š
```python
class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # ä¸‰ä¸ªæ³¨æ„åŠ›å±‚
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
    
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # 1. Masked Self-Attentionï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰
        attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout1(attn_output))
        
        # 2. Cross-Attentionï¼ˆattend to encoderï¼‰
        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout2(attn_output))
        
        # 3. FFN
        ffn_output = self.ffn(x)
        x = self.norm3(x + self.dropout3(ffn_output))
        
        return x
```

---

## äºŒã€å®Œæ•´Transformeræ¨¡å‹

```python
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size,
        tgt_vocab_size,
        d_model=512,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=2048,
        dropout=0.1,
        max_len=5000
    ):
        super().__init__()
        
        # Embeddingå±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)
        
        # Encoder
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
    
    def generate_mask(self, src, tgt):
        # Encoder maskï¼ˆpaddingï¼‰
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        # [batch, 1, 1, src_len]
        
        # Decoder maskï¼ˆpadding + causalï¼‰
        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)
        # [batch, 1, tgt_len, 1]
        
        seq_len = tgt.size(1)
        nopeak_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()
        nopeak_mask = nopeak_mask.to(tgt.device)
        
        tgt_mask = tgt_mask & nopeak_mask
        
        return src_mask, tgt_mask
    
    def encode(self, src, src_mask):
        # Embedding + ä½ç½®ç¼–ç 
        x = self.src_embedding(src) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # é€šè¿‡Encoderå±‚
        for layer in self.encoder_layers:
            x = layer(x, src_mask)
        
        return x
    
    def decode(self, tgt, encoder_output, src_mask, tgt_mask):
        # Embedding + ä½ç½®ç¼–ç 
        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # é€šè¿‡Decoderå±‚
        for layer in self.decoder_layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        
        return x
    
    def forward(self, src, tgt):
        """
        Args:
            src: [batch, src_len]
            tgt: [batch, tgt_len]
        Returns:
            output: [batch, tgt_len, tgt_vocab_size]
        """
        src_mask, tgt_mask = self.generate_mask(src, tgt)
        
        encoder_output = self.encode(src, src_mask)
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)
        
        output = self.fc_out(decoder_output)
        return output
```

---

## ä¸‰ã€è®­ç»ƒæŠ€å·§

### Label Smoothing
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        # pred: [batch * seq_len, vocab_size]
        # target: [batch * seq_len]
        
        vocab_size = pred.size(-1)
        confidence = 1.0 - self.smoothing
        
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (vocab_size - 1))
        true_dist.scatter_(1, target.unsqueeze(1), confidence)
        
        return F.kl_div(F.log_softmax(pred, dim=-1), true_dist, reduction='batchmean')
```

### Warmup Learning Rate
```python
class NoamOpt:
    def __init__(self, d_model, warmup_steps, optimizer):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.optimizer = optimizer
        self._step = 0
    
    def step(self):
        self._step += 1
        lr = self.d_model ** (-0.5) * min(
            self._step ** (-0.5),
            self._step * self.warmup_steps ** (-1.5)
        )
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        self.optimizer.step()
```

---

## å››ã€åº”ç”¨åœºæ™¯

**æœºå™¨ç¿»è¯‘**ï¼š
```python
# è®­ç»ƒ
model = Transformer(src_vocab_size, tgt_vocab_size)
criterion = LabelSmoothingLoss()

for src, tgt in dataloader:
    output = model(src, tgt[:, :-1])  # è¾“å…¥å»æ‰æœ€åä¸€ä¸ªtoken
    loss = criterion(
        output.reshape(-1, output.size(-1)),
        tgt[:, 1:].reshape(-1)  # ç›®æ ‡å»æ‰ç¬¬ä¸€ä¸ªtoken
    )
    loss.backward()
    optimizer.step()
```

**æ–‡æœ¬ç”Ÿæˆ**ï¼ˆGreedy Decodingï¼‰ï¼š
```python
@torch.no_grad()
def generate(model, src, max_len=50):
    model.eval()
    
    src_mask, _ = model.generate_mask(src, src[:, :1])
    encoder_output = model.encode(src, src_mask)
    
    ys = torch.ones(1, 1).fill_(start_token).long()
    
    for i in range(max_len):
        _, tgt_mask = model.generate_mask(src, ys)
        decoder_output = model.decode(ys, encoder_output, src_mask, tgt_mask)
        
        prob = model.fc_out(decoder_output[:, -1])
        next_word = prob.argmax(dim=-1).unsqueeze(0)
        
        ys = torch.cat([ys, next_word], dim=1)
        
        if next_word.item() == end_token:
            break
    
    return ys
```

---

## äº”ã€æ€»ç»“

**Transformerçš„å…³é”®åˆ›æ–°**ï¼š
1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šO(nÂ²)å¤æ‚åº¦æ¢å–å…¨å±€è§†é‡
2. **ä½ç½®ç¼–ç **ï¼šå·§å¦™æ³¨å…¥é¡ºåºä¿¡æ¯
3. **æ®‹å·®è¿æ¥ + LayerNorm**ï¼šç¨³å®šæ·±å±‚ç½‘ç»œè®­ç»ƒ
4. **å¹¶è¡ŒåŒ–**ï¼šGPUå‹å¥½ï¼Œè®­ç»ƒæ•ˆç‡é«˜

**å±€é™æ€§**ï¼š
- åºåˆ—é•¿åº¦çš„å¹³æ–¹å¤æ‚åº¦
- å¯¹ä½ç½®ç¼–ç çš„ä¾èµ–
- ç¼ºä¹å½’çº³åç½®ï¼ˆå¦‚CNNçš„å¹³ç§»ä¸å˜æ€§ï¼‰

**åç»­æ”¹è¿›æ–¹å‘**ï¼š
- **Sparse Attention**ï¼ˆLongformerï¼‰
- **Linear Attention**ï¼ˆPerformerï¼‰
- **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼ˆT5, DeBERTaï¼‰

Transformerå½»åº•æ”¹å˜äº†NLPï¼Œå¹¶é€æ¸æ¸—é€åˆ°CVã€å¤šæ¨¡æ€ã€å¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸï¼Œå ªç§°æ·±åº¦å­¦ä¹ çš„"ä¸‡èƒ½æ¶æ„"ï¼ğŸš€
