---
layout: post
title: "å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯"
subtitle: "ä»CLIPåˆ°GPT-4Vçš„è§†è§‰è¯­è¨€ç†è§£"
date: 2024-10-20
author: "Feng Yu"
header-img: "img/post-bg-recitewords.jpg"
catalog: true
permalink: /2024/10/20/å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯/
tags:
  - å¤§è¯­è¨€æ¨¡å‹
  - å¤šæ¨¡æ€
  - CLIP
  - è§†è§‰è¯­è¨€æ¨¡å‹
categories:
  - Deep Learning
  - Multimodal AI
---

> äººç±»ç†è§£ä¸–ç•Œå¹¶éåªä¾èµ–æ–‡å­—ã€‚æˆ‘ä»¬é€šè¿‡è§†è§‰ã€å¬è§‰ã€è§¦è§‰ç­‰å¤šç§æ„Ÿå®˜ååŒå·¥ä½œï¼Œæ„å»ºå¯¹ä¸–ç•Œçš„å®Œæ•´è®¤çŸ¥ã€‚å¤šæ¨¡æ€å¤§æ¨¡å‹è®©æœºå™¨é¦–æ¬¡èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆå†…å®¹ã€‚

---

## ä¸€ã€CLIPï¼šå¯¹æ¯”å­¦ä¹ çš„çªç ´

### æ ¸å¿ƒæ€æƒ³

**CLIP (Contrastive Language-Image Pre-training)** é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œè®©å›¾åƒå’Œæ–‡æœ¬åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½ã€‚

**å¯¹æ¯”æŸå¤±ï¼ˆInfoNCEï¼‰**ï¼š

$$
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^N \exp(\text{sim}(I_i, T_j) / \tau)}
$$

### PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CLIP(nn.Module):
    def __init__(self, image_encoder, text_encoder, embed_dim=512):
        super().__init__()
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        self.temperature = nn.Parameter(torch.tensor(0.07))
        
        self.image_projection = nn.Linear(image_encoder.output_dim, embed_dim)
        self.text_projection = nn.Linear(text_encoder.output_dim, embed_dim)
    
    def forward(self, images, texts):
        # ç¼–ç 
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(texts)
        
        # æŠ•å½±åˆ°å…±äº«ç©ºé—´
        image_embeds = F.normalize(self.image_projection(image_features), dim=-1)
        text_embeds = F.normalize(self.text_projection(text_features), dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = image_embeds @ text_embeds.T / self.temperature
        
        return logits, logits.T

def clip_loss(logits):
    labels = torch.arange(logits.size(0), device=logits.device)
    loss_i = F.cross_entropy(logits, labels)
    loss_t = F.cross_entropy(logits.T, labels)
    return (loss_i + loss_t) / 2
```

### Zero-Shotåˆ†ç±»

```python
def zero_shot_classify(model, image, class_names):
    texts = [f"a photo of a {c}" for c in class_names]
    
    image_embed = model.encode_image(image)
    text_embeds = model.encode_text(texts)
    
    image_embed = F.normalize(image_embed, dim=-1)
    text_embeds = F.normalize(text_embeds, dim=-1)
    
    logits = (image_embed @ text_embeds.T) / model.temperature
    probs = F.softmax(logits, dim=-1)
    
    return probs[0]
```

---

## äºŒã€BLIPï¼šç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹

### ä¸‰ç§æ¶æ„æ¨¡å¼

1. **Image-Text Contrastive (ITC)**ï¼šç±»ä¼¼CLIP
2. **Image-grounded Text Generation (ITG)**ï¼šå›¾åƒæè¿°ç”Ÿæˆ
3. **Image-Text Matching (ITM)**ï¼šäºŒåˆ†ç±»åˆ¤æ–­

### å›¾åƒæè¿°ç”Ÿæˆ

```python
class BLIP_ITG(nn.Module):
    def __init__(self, image_encoder, text_decoder):
        super().__init__()
        self.image_encoder = image_encoder
        self.text_decoder = text_decoder
        self.cross_attention = nn.MultiheadAttention(embed_dim=768, num_heads=12)
    
    @torch.no_grad()
    def generate_caption(self, image, max_length=50):
        image_embeds = self.image_encoder(image.unsqueeze(0))
        
        generated = [self.text_decoder.bos_token_id]
        for _ in range(max_length):
            text_embeds = self.text_decoder.embed(torch.tensor([generated]))
            
            # è·¨æ¨¡æ€æ³¨æ„åŠ›
            text_embeds, _ = self.cross_attention(text_embeds, image_embeds, image_embeds)
            
            logits = self.text_decoder.lm_head(text_embeds)
            next_token = logits[0, -1].argmax().item()
            
            if next_token == self.text_decoder.eos_token_id:
                break
            generated.append(next_token)
        
        return self.text_decoder.tokenizer.decode(generated)
```

---

## ä¸‰ã€GPT-4Vï¼šç»Ÿä¸€çš„å¤šæ¨¡æ€LLM

### è§†è§‰TokenåŒ–

**æ–¹æ³•1ï¼šLinear Projection** (LLaVA)

```python
class VisualTokenizer(nn.Module):
    def __init__(self, vision_encoder, d_model):
        super().__init__()
        self.encoder = vision_encoder
        self.projection = nn.Linear(vision_encoder.output_dim, d_model)
    
    def forward(self, images):
        features = self.encoder(images)  # [batch, 196, 1024]
        tokens = self.projection(features)  # [batch, 196, 4096]
        return tokens
```

**æ–¹æ³•2ï¼šQ-Former** (BLIP-2)

```python
class QFormer(nn.Module):
    def __init__(self, num_queries=32, d_model=768):
        super().__init__()
        self.queries = nn.Parameter(torch.randn(num_queries, d_model))
        self.cross_attention = nn.TransformerDecoder(...)
    
    def forward(self, image_features):
        batch_size = image_features.size(0)
        queries = self.queries.unsqueeze(0).repeat(batch_size, 1, 1)
        output = self.cross_attention(queries, image_features)
        return output
```

### å¤šæ¨¡æ€Prompt

```python
def build_multimodal_prompt(image, text_prompt):
    visual_tokens = vision_tokenizer(image)
    text_tokens = text_tokenizer(text_prompt)
    
    # å‰ç¼€æ–¹å¼
    input_embeds = torch.cat([visual_tokens, text_tokens], dim=0)
    
    return input_embeds

# ä½¿ç”¨
prompt = "Describe this image in detail:"
response = multimodal_llm.generate(image=img, prompt=prompt)
```

---

## å››ã€è®­ç»ƒæŠ€å·§

### ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜

```python
def hard_negative_mining(image_embeds, text_embeds, num_hard=5):
    sim_matrix = image_embeds @ text_embeds.T
    
    batch_size = sim_matrix.size(0)
    hard_negatives = []
    
    for i in range(batch_size):
        mask = torch.ones_like(sim_matrix[i]).bool()
        mask[i] = False
        
        neg_sims = sim_matrix[i][mask]
        hard_neg_idx = neg_sims.topk(num_hard).indices
        hard_negatives.append(hard_neg_idx)
    
    return hard_negatives
```

### æ¸©åº¦ç¼©æ”¾

```python
class LearnableTemperature(nn.Module):
    def __init__(self, init_value=0.07):
        super().__init__()
        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1 / init_value)))
    
    def forward(self, similarity):
        temperature = torch.clamp(self.logit_scale.exp(), 0.01, 100)
        return similarity * temperature
```

---

## äº”ã€è¯„ä¼°ä¸åŸºå‡†

### ä¸»è¦æ•°æ®é›†

| æ•°æ®é›† | ä»»åŠ¡ | è§„æ¨¡ |
|--------|------|------|
| **MS-COCO** | å›¾åƒæè¿°/VQA | 120Kå›¾ |
| **Flickr30K** | å›¾æ–‡æ£€ç´¢ | 30Kå›¾ |
| **VQAv2** | è§†è§‰é—®ç­” | 1.1Mé—®é¢˜ |
| **LAION-5B** | é¢„è®­ç»ƒ | 50äº¿å›¾æ–‡å¯¹ |

### è¯„ä¼°æŒ‡æ ‡

```python
def calculate_recall(image_embeds, text_embeds, k=[1, 5, 10]):
    sim_matrix = image_embeds @ text_embeds.T
    
    recalls = {}
    for K in k:
        topk_indices = sim_matrix.topk(K, dim=1).indices
        correct = (topk_indices == torch.arange(len(sim_matrix)).unsqueeze(1)).any(dim=1)
        recalls[f'I2T_R@{K}'] = correct.float().mean().item()
    
    return recalls
```

---

## å…­ã€å®æˆ˜ï¼šæ„å»ºç®€æ˜“CLIP

```python
class SimpleCLIP(nn.Module):
    def __init__(self):
        super().__init__()
        
        from torchvision.models import resnet50
        from transformers import AutoModel
        
        resnet = resnet50(pretrained=True)
        self.vision_encoder = nn.Sequential(*list(resnet.children())[:-1])
        self.vision_projection = nn.Linear(2048, 512)
        
        self.text_encoder = AutoModel.from_pretrained('distilbert-base-uncased')
        self.text_projection = nn.Linear(768, 512)
        
        self.temperature = nn.Parameter(torch.tensor(0.07))
    
    def encode_image(self, images):
        features = self.vision_encoder(images).squeeze()
        return F.normalize(self.vision_projection(features), dim=-1)
    
    def encode_text(self, input_ids, attention_mask):
        outputs = self.text_encoder(input_ids, attention_mask)
        pooled = outputs.last_hidden_state[:, 0]
        return F.normalize(self.text_projection(pooled), dim=-1)

# è®­ç»ƒ
model = SimpleCLIP()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

for images, texts in dataloader:
    image_embeds = model.encode_image(images)
    text_embeds = model.encode_text(texts['input_ids'], texts['attention_mask'])
    
    logits = (image_embeds @ text_embeds.T) / model.temperature
    
    labels = torch.arange(len(images), device=logits.device)
    loss = (F.cross_entropy(logits, labels) + 
            F.cross_entropy(logits.T, labels)) / 2
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## ä¸ƒã€æ€»ç»“

**å¤šæ¨¡æ€AIçš„æ ¸å¿ƒ**ï¼š
1. **å¯¹é½æ˜¯å…³é”®**ï¼šCLIPç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½å›¾æ–‡
2. **æ¶æ„é€‰æ‹©**ï¼šç¼–ç å™¨ vs ç¼–è§£ç å™¨ vs ç»Ÿä¸€LLM
3. **æ•°æ®è§„æ¨¡**ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒæ˜¯åŸºç¡€

**å‰æ²¿æ–¹å‘**ï¼š
- **ImageBind**ï¼šå…­æ¨¡æ€ç»Ÿä¸€ç©ºé—´
- **Any-to-Anyç”Ÿæˆ**ï¼šä»»æ„æ¨¡æ€è¾“å…¥è¾“å‡º
- **ä¸–ç•Œæ¨¡å‹**ï¼šç‰©ç†æ¨ç†ä¸3Dç†è§£

å¤šæ¨¡æ€å¤§æ¨¡å‹æ­£åœ¨æˆä¸ºAIçš„ä¸‹ä¸€ä¸ªåˆ¶é«˜ç‚¹ï¼ğŸš€
